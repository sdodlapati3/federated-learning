# FL Research: Federated Learning with Differential Privacy

[![CI](https://github.com/sdodlapati3/federated-learning/actions/workflows/ci.yml/badge.svg)](https://github.com/sdodlapati3/federated-learning/actions/workflows/ci.yml)
[![Python 3.8+](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://python.org/)
[![PyTorch 2.0+](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![Flower 1.24+](https://img.shields.io/badge/Flower-1.24+-orange.svg)](https://flower.ai/)
[![Opacus 1.5+](https://img.shields.io/badge/Opacus-1.5+-green.svg)](https://opacus.ai/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

A comprehensive library and learning resource for **Federated Learning (FL)** with **Differential Privacy (DP)**. Built on top of [Flower](https://flower.ai/), [PyTorch](https://pytorch.org/), and [Opacus](https://opacus.ai/).

## ğŸ¯ Features

- **Reusable FL Components**: Models, data partitioners, strategies, and privacy tools
- **Multiple FL Strategies**: FedAvg, FedProx, SCAFFOLD with variance reduction
- **Differential Privacy**: RDP accounting, noise mechanisms, Opacus integration
- **Non-IID Data Simulation**: Dirichlet, pathological, and shard-based partitioning
- **Experiment Tracking**: Metrics, checkpointing, and reproducibility utilities
- **Comprehensive Documentation**: Learning guides, algorithm notes, framework comparisons
- **105 Unit Tests**: Extensive test coverage with CI/CD pipeline

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/sdodlapati3/federated-learning.git
cd federated-learning

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/Mac

# Install the library in development mode
pip install -e .

# Or install dependencies only
pip install -r requirements.txt
```

## ğŸš€ Quick Start

### Using the Library

```python
from fl_research.models import ModelRegistry, CIFAR10CNN
from fl_research.data import load_cifar10, DirichletPartitioner
from fl_research.privacy import PrivacyAccountant
from fl_research.strategies import SCAFFOLDServer, StandaloneSCAFFOLDClient
from fl_research.utils import set_seed, get_device, MetricsTracker

# Set up reproducibility
set_seed(42)
device = get_device()

# Load data with non-IID partitioning
train_data, test_data = load_cifar10()
partitioner = DirichletPartitioner(num_clients=10, alpha=0.5)
partitions = partitioner.partition(train_data)

# Create model from registry
model = ModelRegistry.create('cifar10cnn').to(device)

# Track metrics
tracker = MetricsTracker()
tracker.log_round({'round': 1, 'accuracy': 0.85, 'loss': 0.45})
```

### Available Models

```python
from fl_research.models import ModelRegistry

# List all available models
print(ModelRegistry.list())
# ['simplecnn', 'cifar10cnn', 'cifar10cnn_opacus', 'resnet_small', 'mlp', 'twolayer_mlp']

# Create models
cnn = ModelRegistry.create('cifar10cnn')
dp_cnn = ModelRegistry.create('cifar10cnn_opacus')  # DP-compatible (GroupNorm)
mlp = ModelRegistry.create('mlp', input_dim=784, hidden_dims=[256, 128], num_classes=10)
```

### Data Partitioning

```python
from fl_research.data import IIDPartitioner, DirichletPartitioner, ShardPartitioner

# IID partitioning (uniform distribution)
iid = IIDPartitioner(num_clients=10)

# Non-IID with Dirichlet distribution (alpha controls heterogeneity)
dirichlet = DirichletPartitioner(num_clients=10, alpha=0.1)  # Very heterogeneous
dirichlet = DirichletPartitioner(num_clients=10, alpha=1.0)  # Moderate

# Shard-based partitioning
shard = ShardPartitioner(num_clients=10, shards_per_client=2)

# Apply to dataset
partitions = dirichlet.partition(train_dataset)  # Returns list of index lists
```

### Privacy Accounting

```python
from fl_research.privacy import PrivacyAccountant, get_privacy_spent

# Create accountant with budget
accountant = PrivacyAccountant(target_epsilon=10.0, target_delta=1e-5)

# Track privacy consumption
for round in range(100):
    accountant.step(noise_multiplier=1.0, sample_rate=0.01, num_steps=10)
    
    if accountant.is_budget_exceeded():
        print(f"Budget exceeded at round {round}")
        break

# Get current epsilon
epsilon = accountant.get_epsilon()
```

### FL Strategies

```python
from fl_research.strategies import SCAFFOLDServer, StandaloneSCAFFOLDClient

# SCAFFOLD for variance reduction
server = SCAFFOLDServer(model, device)
client = StandaloneSCAFFOLDClient(client_id=0, dataloader=loader, device=device)
client.initialize_control_variate(model)

# Train with gradient correction
delta_w, delta_c, count = client.train(model, server.global_control, epochs=5, lr=0.1)
server.aggregate([delta_w], [delta_c], [count], total_clients=10)
```

## ğŸ“ Project Structure

```
flower-federated-learning/
â”œâ”€â”€ src/fl_research/           # Main library
â”‚   â”œâ”€â”€ models/                # CNN, MLP, ResNet models
â”‚   â”‚   â”œâ”€â”€ cnn.py            # SimpleCNN, CIFAR10CNN, CIFAR10CNNOpacus
â”‚   â”‚   â”œâ”€â”€ mlp.py            # MLP, TwoLayerMLP
â”‚   â”‚   â””â”€â”€ registry.py       # ModelRegistry factory
â”‚   â”œâ”€â”€ data/                  # Data loading and partitioning
â”‚   â”‚   â”œâ”€â”€ loaders.py        # CIFAR-10, MNIST, Fashion-MNIST
â”‚   â”‚   â””â”€â”€ partitioners.py   # IID, Dirichlet, Shard partitioners
â”‚   â”œâ”€â”€ privacy/               # Differential privacy
â”‚   â”‚   â”œâ”€â”€ accountant.py     # RDP-based privacy accounting
â”‚   â”‚   â”œâ”€â”€ mechanisms.py     # Gaussian, Laplace noise
â”‚   â”‚   â””â”€â”€ opacus_utils.py   # Opacus integration helpers
â”‚   â”œâ”€â”€ strategies/            # FL algorithms
â”‚   â”‚   â”œâ”€â”€ fedavg.py         # Federated Averaging
â”‚   â”‚   â”œâ”€â”€ fedprox.py        # FedProx with proximal term
â”‚   â”‚   â””â”€â”€ scaffold.py       # SCAFFOLD with control variates
â”‚   â””â”€â”€ utils/                 # Utilities
â”‚       â”œâ”€â”€ config.py         # YAML/JSON configuration
â”‚       â”œâ”€â”€ metrics.py        # MetricsTracker
â”‚       â”œâ”€â”€ checkpointing.py  # Model checkpoints
â”‚       â””â”€â”€ reproducibility.py # Seeds, device detection
â”œâ”€â”€ tests/                     # Test suite (105 tests)
â”‚   â”œâ”€â”€ test_data.py          # Data loading tests
â”‚   â”œâ”€â”€ test_models.py        # Model registry tests
â”‚   â”œâ”€â”€ test_privacy.py       # Privacy accounting tests
â”‚   â”œâ”€â”€ test_utils.py         # Utility tests
â”‚   â”œâ”€â”€ test_strategies.py    # FedAvg, FedProx, SCAFFOLD tests
â”‚   â””â”€â”€ test_integration.py   # End-to-end workflow tests
â”œâ”€â”€ examples/                  # Example experiments
â”‚   â”œâ”€â”€ dp/                   # Differential privacy experiments
â”‚   â”œâ”€â”€ scaffold/             # SCAFFOLD vs FedAvg comparison
â”‚   â”œâ”€â”€ fedprox/              # FedProx experiments
â”‚   â”œâ”€â”€ minimal_example.py    # Quick runnable demo
â”‚   â””â”€â”€ ...                   # Additional examples
â”œâ”€â”€ docs/                      # ğŸ“š Documentation
â”‚   â”œâ”€â”€ FEDERATED_LEARNING_GUIDE.md  # Comprehensive FL learning guide
â”‚   â”œâ”€â”€ DP_VARIANTS_NOTES.md         # DP variants explained
â”‚   â”œâ”€â”€ DISTRIBUTED_OPTIMIZATION_NOTES.md  # Optimization theory
â”‚   â”œâ”€â”€ HPC_SCALING_REPORT.md        # HPC scaling patterns
â”‚   â”œâ”€â”€ FEDML_COMPARISON.md          # Framework comparison
â”‚   â””â”€â”€ COMPLETION_REPORT.md         # Project summary
â”œâ”€â”€ .github/workflows/ci.yml   # CI/CD pipeline
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â””â”€â”€ requirements.txt
```

## ğŸ“Š Experiments

### Differential Privacy Experiments

```bash
cd examples/dp
python run_standalone_experiments_refactored.py
```

Compares IID vs Non-IID data with varying privacy levels (Îµ = 2, 4, 8).

### SCAFFOLD vs FedAvg

```bash
cd examples/scaffold
python scaffold_implementation_refactored.py
```

Demonstrates variance reduction with control variates on heterogeneous data.

### FedProx Comparison

```bash
cd examples/fedprox
python fedprox_implementation_refactored.py
```

Shows proximal term benefits for client drift mitigation.

## ğŸ§ª Testing

```bash
# Run all tests (105 tests)
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=fl_research --cov-report=html

# Run specific test files
pytest tests/test_strategies.py -v  # Strategy tests
pytest tests/test_integration.py -v  # End-to-end tests
```

## ğŸ“š Learning Path

This repository follows a structured learning journey:

| Week | Topic | Key Concepts |
|------|-------|--------------|
| 1-2 | Flower Basics | Client/Server, FedAvg, Simulation |
| 3 | Differential Privacy | Îµ-Î´ DP, RDP, Opacus integration |
| 4 | DP + FL | Privacy-utility tradeoffs, composition |
| 5-6 | Advanced Strategies | FedProx, SCAFFOLD, Non-IID handling |

## ğŸ”§ Configuration

```python
from fl_research.utils import Config, save_config, load_config

# Create experiment config
config = Config(
    num_rounds=100,
    num_clients=10,
    batch_size=32,
    learning_rate=0.01,
    strategy='scaffold',
    privacy={'epsilon': 10.0, 'delta': 1e-5}
)

# Save and load
save_config(config, 'experiment.yaml')
loaded = load_config('experiment.yaml')
```

## ğŸ“š Documentation

See the [`docs/`](docs/) folder for comprehensive learning resources:

| Document | Description |
|----------|-------------|
| [FEDERATED_LEARNING_GUIDE.md](docs/FEDERATED_LEARNING_GUIDE.md) | ğŸ“– Complete FL+DP learning guide with citations |
| [DP_VARIANTS_NOTES.md](docs/DP_VARIANTS_NOTES.md) | Approximate, RÃ©nyi, Local DP explained |
| [DISTRIBUTED_OPTIMIZATION_NOTES.md](docs/DISTRIBUTED_OPTIMIZATION_NOTES.md) | Optimization theory for FL |
| [HPC_SCALING_REPORT.md](docs/HPC_SCALING_REPORT.md) | HPC scaling patterns |
| [FEDML_COMPARISON.md](docs/FEDML_COMPARISON.md) | Framework comparison guide |

## ğŸ“– References

### Papers
- [FedAvg](https://arxiv.org/abs/1602.05629) - Communication-Efficient Learning
- [FedProx](https://arxiv.org/abs/1812.06127) - Heterogeneous Federated Optimization
- [SCAFFOLD](https://arxiv.org/abs/1910.06378) - Stochastic Controlled Averaging
- [DP-SGD](https://arxiv.org/abs/1607.00133) - Deep Learning with Differential Privacy
- [Awesome Federated Learning](https://github.com/FedML-AI/FedML/blob/master/research/Awesome-Federated-Learning.md) - 500+ papers

### Documentation
- [Flower Framework](https://flower.ai/docs/framework/)
- [Opacus Library](https://opacus.ai/)
- [PyTorch](https://pytorch.org/docs/)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit issues and pull requests.

## ğŸ“ License

MIT License - See [LICENSE](LICENSE) for details.

---

**Built with â¤ï¸ for the Federated Learning research community**
